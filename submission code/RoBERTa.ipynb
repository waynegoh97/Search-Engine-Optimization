{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c596141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d829279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          review_content sentiment\n",
      "0      I feel like I received a different product tha...  negative\n",
      "6867   We won't get a chance to play the game. I'm re...  negative\n",
      "6866   this toy looks great,  it works great,  but th...  negative\n",
      "28036  Not the item that is advertised. I ordered twi...  negative\n",
      "6863   The box of 40 colors only had 30 colors; 10 co...  negative\n",
      "...                                                  ...       ...\n",
      "32731  Although I cannot believe how much they charge...  positive\n",
      "32743             My 3 year old son loves these animals!  positive\n",
      "32729  Zero complaints. This is the uno I remember as...  positive\n",
      "32757  Melissa & Doug stamps are quality stamps geare...  positive\n",
      "32760  Very impressive figure with extremely nice pac...  positive\n",
      "\n",
      "[14849 rows x 2 columns]\n",
      "                                         review_content sentiment\n",
      "0                                   Exactly as expected  positive\n",
      "1     The kids with special needs love this game...w...  positive\n",
      "2     These are basically lit toys...but for the pan...  positive\n",
      "3     The media could not be loaded. So I bought the...  positive\n",
      "4     The media could not be loaded. This Fisher-Pri...  positive\n",
      "...                                                 ...       ...\n",
      "3671  My granddaughter enjoyed making fairy house an...  positive\n",
      "3672  I mean it’s Melissa & Doug. It’s a quality ite...  positive\n",
      "3673  Where do I start? I'll keep it simple. If you ...  positive\n",
      "3674  It broke as soon as I tried it on and No longe...  negative\n",
      "3675  I played this game all the time as a child and...  positive\n",
      "\n",
      "[3676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_csv('test_data.csv')\n",
    "train_csv = pd.read_csv('train_data.csv')\n",
    "train_csv = train_csv.sort_values('sentiment')\n",
    "train_csv = train_csv.drop(train_csv[train_csv.sentiment == 'positive'].index[int(train_csv.count()['sentiment']/5.5):])\n",
    "train_df = train_csv[['review_content','sentiment']]\n",
    "test_df = test_csv[['review_content','Annotator_1']]\n",
    "test_df = test_df.rename(columns={'Annotator_1':'sentiment'})\n",
    "train_df = train_df.dropna()\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "#Changing labels to numbers\n",
    "encode_lb = {'negative':0, 'neutral':1, 'positive':2}\n",
    "train_df['sentiment_lb'] = train_df['sentiment'].map(encode_lb)\n",
    "test_df['sentiment_lb'] = test_df['sentiment'].map(encode_lb)\n",
    "train_df['review_content'] = train_df['review_content'].apply(lambda x:\" \".join(x.split()[:512]))\n",
    "test_df['review_content'] = test_df['review_content'].apply(lambda x:\" \".join(x.split()[:512]))\n",
    "train_df.to_csv('roberta_train_processed.csv',index=False)\n",
    "test_df.to_csv('roberta_test_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae1519c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data_path = './roberta/data'\n",
    "output_path = './roberta/output'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "# Set tokenizer hyperparameters.\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Define columns to read.\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   truncate_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "\n",
    "fields = {'review_content' : ('review_content', text_field), 'sentiment_lb' : ('sentiment_lb', label_field)}\n",
    "\n",
    "\n",
    "# Read preprocessed CSV into TabularDataset and split it into train, test and valid.\n",
    "train_data, valid_data = TabularDataset(path=f\"roberta_train_processed.csv\", format='CSV', fields=fields, \n",
    "                        skip_header=False).split(split_ratio=[0.8, 0.2], stratified=True, strata_field='sentiment_lb')\n",
    "test_data = TabularDataset(path=f\"roberta_test_processed.csv\", format='CSV', fields=fields, skip_header=False)\n",
    "\n",
    "# Create train and validation iterators.\n",
    "train_iter, valid_iter = BucketIterator.splits((train_data, valid_data),\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               device=device,\n",
    "                                               shuffle=True,\n",
    "                                               sort_key=lambda x: len(x.review_content), \n",
    "                                               sort=True, \n",
    "                                               sort_within_batch=False)\n",
    "\n",
    "# Test iterator, no shuffling or sorting required.\n",
    "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e74a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66a3588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1 = torch.nn.Linear(768, 64)\n",
    "        self.bn1 = torch.nn.LayerNorm(64)\n",
    "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2 = torch.nn.Linear(64, 3) #changed output to 3 since we have 3 classes to predict\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        x = self.d1(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.Tanh()(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15fc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training classifier for feature extraction\n",
    "def pretrain(model, \n",
    "             optimizer, \n",
    "             train_iter, \n",
    "             valid_iter, \n",
    "             scheduler,\n",
    "             num_epochs,\n",
    "             valid_period = len(train_iter)):\n",
    "    \n",
    "    # Pretrain linear layers, do not train bert\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0   \n",
    "    global_step = 0  \n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "            \n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "   \n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], PT Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "    \n",
    "    # Set bert parameters back to trainable\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print('Pre-training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3382dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training classifier for fine-tuning\n",
    "def train(model, optimizer, train_iter, valid_iter, output_path, scheduler, num_epochs, valid_period = len(train_iter)):\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('Inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    global_steps_list = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    save_checkpoint(output_path + '/RoBERTa.pkl', model, best_valid_loss)\n",
    "                    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "                        \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "                model.train()\n",
    "    \n",
    "    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615b9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/SEANGLIDET/CZ1016/base/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start pretraining ==============================\n",
      "Epoch [1/6], global step [743/4458], PT Loss: 1.0932, Val Loss: 1.0546\n",
      "Epoch [2/6], global step [1486/4458], PT Loss: 1.0486, Val Loss: 1.0463\n",
      "Epoch [3/6], global step [2229/4458], PT Loss: 1.0412, Val Loss: 1.0430\n",
      "Epoch [4/6], global step [2972/4458], PT Loss: 1.0353, Val Loss: 1.0384\n",
      "Epoch [5/6], global step [3715/4458], PT Loss: 1.0335, Val Loss: 1.0326\n",
      "Epoch [6/6], global step [4458/4458], PT Loss: 1.0361, Val Loss: 1.0275\n",
      "Pre-training done!\n",
      "======================= Start training =================================\n",
      "Epoch [1/12], global step [743/8916], Train Loss: 0.9268, Valid Loss: 0.5672\n",
      "Epoch [2/12], global step [1486/8916], Train Loss: 0.5584, Valid Loss: 0.4899\n",
      "Epoch [4/12], global step [2972/8916], Train Loss: 0.4622, Valid Loss: 0.4606\n",
      "Epoch [5/12], global step [3715/8916], Train Loss: 0.4465, Valid Loss: 0.4590\n",
      "Epoch [6/12], global step [4458/8916], Train Loss: 0.4281, Valid Loss: 0.4619\n",
      "Epoch [7/12], global step [5201/8916], Train Loss: 0.4065, Valid Loss: 0.4676\n",
      "Epoch [8/12], global step [5944/8916], Train Loss: 0.4007, Valid Loss: 0.4676\n",
      "Epoch [9/12], global step [6687/8916], Train Loss: 0.3842, Valid Loss: 0.4699\n",
      "Epoch [10/12], global step [7430/8916], Train Loss: 0.3735, Valid Loss: 0.4716\n",
      "Epoch [11/12], global step [8173/8916], Train Loss: 0.3733, Valid Loss: 0.4694\n",
      "Epoch [12/12], global step [8916/8916], Train Loss: 0.3665, Valid Loss: 0.4676\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "NUM_EPOCHS = 6\n",
    "steps_per_epoch = len(train_iter)\n",
    "\n",
    "model = ROBERTAClassifier(0.4)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "print(\"======================= Start pretraining ==============================\")\n",
    "\n",
    "pretrain(model,optimizer,train_iter,valid_iter,scheduler,NUM_EPOCHS)\n",
    "\n",
    "NUM_EPOCHS = 12\n",
    "print(\"======================= Start training =================================\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*2, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model, optimizer,train_iter, valid_iter, output_path, scheduler, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "488eca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in test_loader:\n",
    "                mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                \n",
    "                output = model(source, attention_mask=mask)\n",
    "\n",
    "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
    "                y_true.extend(target.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[0,1,2], digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a72223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8568    0.8797    0.8681       748\n",
      "           1     0.5357    0.5636    0.5493       346\n",
      "           2     0.9642    0.9500    0.9571      2582\n",
      "\n",
      "    accuracy                         0.8993      3676\n",
      "   macro avg     0.7856    0.7978    0.7915      3676\n",
      "weighted avg     0.9020    0.8993    0.9006      3676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "model = ROBERTAClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "load_checkpoint(output_path + '/RoBERTa.pkl', model)\n",
    "\n",
    "evaluate(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdf352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
