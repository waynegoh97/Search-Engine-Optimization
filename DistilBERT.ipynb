{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yongy\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q transformers\n",
    "#!pip install tensorflow_datasets\n",
    "#!pip install torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n",
      "keras version: 2.6.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Default GPU Device: {}'.format(torch.device(\"cuda:0\")))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(\"keras version: {0}\".format(keras.__version__))\n",
    "print(\"Num GPUs Available: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# can be up to 512 for BERT\n",
    "max_length = 256\n",
    "batch_size = 18\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 3e-5\n",
    "# we will do just 2 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 2\n",
    "\n",
    "#Assign tokenizer object to the tokenizer class\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=3)\n",
    "#Assign tokenizer object to the tokenizer class\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_content    False\n",
      "sentiment         False\n",
      "dtype: bool\n",
      "review_content    False\n",
      "sentiment         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATASETS\n",
    "\n",
    "\"\"\"\n",
    "test_csv = pd.read_csv('test_data.csv') \n",
    "train_csv = pd.read_csv('train_data.csv') #sentiment\n",
    "train_csv = train_csv.sort_values('sentiment')\n",
    "train_csv = train_csv.drop(train_csv[train_csv.sentiment == 'positive'].index[int(train_csv.count()['sentiment']/5.5):])\n",
    "ds_train = train_csv[['review_content','sentiment']]\n",
    "ds_test = test_csv[['review_content','Annotator_1']]\n",
    "ds_test = ds_test.rename(columns={'Annotator_1':'sentiment'})\n",
    "ds_train = ds_train.dropna()\n",
    "print(ds_train.isnull().any())\n",
    "print(ds_test.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "possible_labels = ds_train.sentiment.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "print(label_dict)\n",
    "\n",
    "ds_train['label'] = ds_train.sentiment.replace(label_dict)\n",
    "ds_test['label'] = ds_test.sentiment.replace(label_dict)\n",
    "y_train = to_categorical(ds_train.label)\n",
    "y_test = to_categorical(ds_test.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input (takes some time) \n",
    "# here tokenizer using from bert-base-uncased\n",
    "x_train = tokenizer(\n",
    "    text=ds_train.review_content.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "x_test = tokenizer(\n",
    "    text=ds_test.review_content.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x_train['input_ids']\n",
    "attention_mask = x_train['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = model(input_ids,attention_mask = input_mask)[0] \n",
    "out = Dense(128, activation='relu')(embeddings)\n",
    "out = tf.keras.layers.Dropout(0.2)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "y = Dense(3,activation = 'sigmoid')(out)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=learning_rate, # this learning rate is for bert model , taken from huggingface website \n",
    "    epsilon=1e-08, decay=0.01, clipnorm=1.0)\n",
    "# Set loss and metrics\n",
    "loss =CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1238/1238 [==============================] - 301s 243ms/step - loss: 0.7233 - balanced_accuracy: 0.7230 - val_loss: 0.3975 - val_balanced_accuracy: 0.8496\n",
      "Epoch 2/2\n",
      "1238/1238 [==============================] - 408s 330ms/step - loss: 0.5640 - balanced_accuracy: 0.7721 - val_loss: 0.3500 - val_balanced_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "bert_search = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_train,\n",
    "    validation_data = (\n",
    "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "    ),\n",
    "  epochs=number_of_epochs,\n",
    "    batch_size=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yongy\\OneDrive\\Documents\\GitHub\\Classifier\\SEwithBERT.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yongy/OneDrive/Documents/GitHub/Classifier/SEwithBERT.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# model.save('BERTmodel')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yongy/OneDrive/Documents/GitHub/Classifier/SEwithBERT.ipynb#ch0000010?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39mBERT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yongy/OneDrive/Documents/GitHub/Classifier/SEwithBERT.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaved model to disk\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# model.save('BERTmodel')\n",
    "model.save_pretrained(\"BERT\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, False), {})\n\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 100), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, False), {})\n\nMore specifically: Substructure \"type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids')\" is not\nEntire first structure:\n(({'input_ids': .}, ., ., ., ., ., ., ., .), {})\nEntire second structure:\n((., ., ., ., ., ., ., ., .), {})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:527\u001b[0m, in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=525'>526</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=526'>527</a>\u001b[0m   _pywrap_utils\u001b[39m.\u001b[39;49mAssertSameStructure(nest1, nest2, check_types,\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=527'>528</a>\u001b[0m                                     expand_composites)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=528'>529</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, False), {})\n\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 100), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, False), {})\n\nMore specifically: Substructure \"type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids')\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yongy\\OneDrive\\Documents\\GitHub\\Classifier\\SEwithBERT.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yongy/OneDrive/Documents/GitHub/Classifier/SEwithBERT.ipynb#ch0000013?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mBERTmodel\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\keras\\saving\\save.py:205\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=202'>203</a>\u001b[0m       filepath \u001b[39m=\u001b[39m path_to_string(filepath)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=203'>204</a>\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=204'>205</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39;49mload(filepath, \u001b[39mcompile\u001b[39;49m, options)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=206'>207</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=207'>208</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mUnable to load model. Filepath is not an hdf5 file (or h5py is not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/save.py?line=208'>209</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mavailable) or SavedModel.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:143\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=139'>140</a>\u001b[0m loaded \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mload_partial(path, nodes_to_load, options\u001b[39m=\u001b[39moptions)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=141'>142</a>\u001b[0m \u001b[39m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=142'>143</a>\u001b[0m keras_loader\u001b[39m.\u001b[39;49mfinalize_objects()\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=143'>144</a>\u001b[0m keras_loader\u001b[39m.\u001b[39mdel_tracking()\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=145'>146</a>\u001b[0m model \u001b[39m=\u001b[39m loaded[\u001b[39m'\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:640\u001b[0m, in \u001b[0;36mKerasObjectLoader.finalize_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=636'>637</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=637'>638</a>\u001b[0m     layers_revived_from_config\u001b[39m.\u001b[39mappend(node)\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=639'>640</a>\u001b[0m _finalize_saved_model_layers(layers_revived_from_saved_model)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=640'>641</a>\u001b[0m _finalize_config_layers(layers_revived_from_config)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=642'>643</a>\u001b[0m \u001b[39m# Initialize graph networks, now that layer dependencies have been resolved.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:837\u001b[0m, in \u001b[0;36m_finalize_saved_model_layers\u001b[1;34m(layers)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=834'>835</a>\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=835'>836</a>\u001b[0m \u001b[39mif\u001b[39;00m call_fn\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=836'>837</a>\u001b[0m   args, kwargs \u001b[39m=\u001b[39m infer_inputs_from_restored_call_function(call_fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=837'>838</a>\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(args)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=838'>839</a>\u001b[0m   inputs \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:1174\u001b[0m, in \u001b[0;36minfer_inputs_from_restored_call_function\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=1171'>1172</a>\u001b[0m \u001b[39mfor\u001b[39;00m concrete \u001b[39min\u001b[39;00m fn\u001b[39m.\u001b[39mconcrete_functions[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m   <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=1172'>1173</a>\u001b[0m   spec2 \u001b[39m=\u001b[39m concrete\u001b[39m.\u001b[39mstructured_input_signature\n\u001b[1;32m-> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=1173'>1174</a>\u001b[0m   spec \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(common_spec, spec, spec2)\n\u001b[0;32m   <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/keras/saving/saved_model/load.py?line=1174'>1175</a>\u001b[0m \u001b[39mreturn\u001b[39;00m spec\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:862\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=856'>857</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=857'>858</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mOnly valid keyword arguments are `check_types` and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=858'>859</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`expand_composites`, not: `\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m`, `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(kwargs\u001b[39m.\u001b[39mkeys())))\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=860'>861</a>\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structure[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=861'>862</a>\u001b[0m   assert_same_structure(structure[\u001b[39m0\u001b[39;49m], other, check_types\u001b[39m=\u001b[39;49mcheck_types,\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=862'>863</a>\u001b[0m                         expand_composites\u001b[39m=\u001b[39;49mexpand_composites)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=864'>865</a>\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=865'>866</a>\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Deeplearningenv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:532\u001b[0m, in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=529'>530</a>\u001b[0m str1 \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(map_structure(\u001b[39mlambda\u001b[39;00m _: _DOT, nest1))\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=530'>531</a>\u001b[0m str2 \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(map_structure(\u001b[39mlambda\u001b[39;00m _: _DOT, nest2))\n\u001b[1;32m--> <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=531'>532</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=532'>533</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mEntire first structure:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=533'>534</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mEntire second structure:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yongy/.conda/envs/Deeplearningenv/lib/site-packages/tensorflow/python/util/nest.py?line=534'>535</a>\u001b[0m               \u001b[39m%\u001b[39m (\u001b[39mstr\u001b[39m(e), str1, str2))\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}, None, None, None, None, None, None, None, False), {})\n\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 100), dtype=tf.int32, name='attention_mask'), None, None, None, None, None, None, False), {})\n\nMore specifically: Substructure \"type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_ids')\" is not\nEntire first structure:\n(({'input_ids': .}, ., ., ., ., ., ., ., .), {})\nEntire second structure:\n((., ., ., ., ., ., ., ., .), {})"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('BERTmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3111919 , 0.13600692, 0.8138756 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "predicted_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.98      0.75       748\n",
      "           1       0.50      0.00      0.01       346\n",
      "           2       0.97      0.93      0.95      2582\n",
      "\n",
      "    accuracy                           0.85      3676\n",
      "   macro avg       0.69      0.64      0.57      3676\n",
      "weighted avg       0.85      0.85      0.82      3676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted = np.argmax(predicted_raw, axis = 1)\n",
    "y_true = ds_test.label\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_predicted))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0986f77b5bdf0fa34ec660bfcb8e1462e7d7eed3068c7b5d1771e76fa1438ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('Deeplearningenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
