{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25247f8",
   "metadata": {},
   "source": [
    "# Creating Sarcasm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbb950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting ./sarcasm-detection/Regular.rar ...\n",
      "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -o./sarcasm-detection -- ./sarcasm-detection/Regular.rar\n",
      "patool: ... ./sarcasm-detection/Regular.rar extracted to `./sarcasm-detection'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./sarcasm-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sarcasm corpus: https://github.com/ef2020/SarcasmAmazonReviewsCorpus\n",
    "import patoolib\n",
    "patoolib.extract_archive(\"./sarcasm-detection/Regular.rar\", outdir=\"./sarcasm-detection\")\n",
    "patoolib.extract_archive(\"./sarcasm-detection/Ironic.rar\", outdir=\"./sarcasm-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fd15e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review  sarcasm_lb\n",
      "0    I can not believe my eyes, or my ears.... The ...           1\n",
      "1    Journalist Weisberg here cashes in on the curr...           1\n",
      "2    When my friend purchased and forced me to watc...           1\n",
      "3    Does the publisher seriously think anyone is g...           1\n",
      "4    This toy would be a lot more realistic with ab...           1\n",
      "..                                                 ...         ...\n",
      "432  Ever notice in some of the reviews of this and...           1\n",
      "433  This is is some of the best dick I've ever had...           1\n",
      "434  if your not to old to lose most of your olfact...           1\n",
      "435  I mean, I always wanted my crotch and my hands...           1\n",
      "436  Seems like a good quality product, except that...           1\n",
      "\n",
      "[437 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "#Extract sarcastic reviews\n",
    "review = []\n",
    "file_name = []\n",
    "for file in os.listdir(\"./sarcasm-detection/Ironic\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_name.append(file)\n",
    "for size in range(len(file_name)):\n",
    "    f = open(\"./sarcasm-detection/Ironic/\"+file_name[size])\n",
    "    soup = f.read()\n",
    "    b = bs.BeautifulSoup(soup)\n",
    "    review.append(b.find('review').text)\n",
    "\n",
    "lb = [1 for k in range(len(review))]\n",
    "filtered_review = []\n",
    "for x in review:\n",
    "    filtered_review.append(x.replace(\"\\n\", \"\"))\n",
    "df = pd.DataFrame(filtered_review, columns=['review'])\n",
    "df['sarcasm_lb'] = lb\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87134018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review  sarcasm_lb\n",
      "0    They are amazingly thin, which is quite impres...           0\n",
      "1    First, I am not in the expected fan base for J...           0\n",
      "2    I shot this using the Kodak PlaySport while sn...           0\n",
      "3    The keyboard on this notebook is the highlight...           0\n",
      "4    With a little peanut butter and jelly, these t...           0\n",
      "..                                                 ...         ...\n",
      "812  This product is fantastic!  If you're looking ...           0\n",
      "813  The Scent of Rain and Lightning by Nancy Picka...           0\n",
      "814  Ok, I am an old time monkey Island game fan.  ...           0\n",
      "815  I got this as a Christmas present for my broth...           0\n",
      "816  this stand does not work with either of my bik...           0\n",
      "\n",
      "[817 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Extract normal reviews\n",
    "review = []\n",
    "file_name = []\n",
    "for file in os.listdir(\"./sarcasm-detection/Regular\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_name.append(file)\n",
    "for size in range(len(file_name)):\n",
    "    f = open(\"./sarcasm-detection/Regular/\"+file_name[size])\n",
    "    soup = f.read()\n",
    "    b = bs.BeautifulSoup(soup)\n",
    "    review.append(b.find('review').text)\n",
    "\n",
    "lb = [0 for k in range(len(review))]\n",
    "filtered_review = []\n",
    "for x in review:\n",
    "    filtered_review.append(x.replace(\"\\n\", \"\"))\n",
    "ndf = pd.DataFrame(filtered_review, columns=['review'])\n",
    "ndf['sarcasm_lb'] = lb\n",
    "print(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23b2a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both df to create a csv dataset\n",
    "save_df = pd.concat([df,ndf])\n",
    "save_df.to_csv(\"./sarcasm-detection/sarcasm-dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b74da3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(save_df, test_size=0.2)\n",
    "train.to_csv(\"./sarcasm-detection/train_data.csv\",index=False)\n",
    "test.to_csv(\"./sarcasm-detection/test_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e4401",
   "metadata": {},
   "source": [
    "# Roberta for sarcasm detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37d878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8cfb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_df = pd.read_csv(\"./sarcasm-detection/sarcasm-dataset.csv\",header=0)\n",
    "new_df = train_df[['review', 'sarcasm_lb']]\n",
    "\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1e6fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = dataframe['review']\n",
    "        self.targets = dataframe['sarcasm_lb']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10825ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Tokenize(train_df, tokenizer, MAX_LEN)\n",
    "trainloader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle = True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e03a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Roberta, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "PATH = './sarcasm-detection/roberta-large.pt'\n",
    "model = Roberta()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "m = nn.Sigmoid()\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(trainloader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(m(outputs), targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
